# Input: the confidences.csv file generated by the batch_label_images.py script.
# https://github.com/georgeslabreche/opssat-smartcam/blob/main/train/batch_label_images.py
#
# Output: a plot that reveals how many images will be discarded from the test set
# with respect to the value set for the prediction confidence threshold.

library(here)
library(RColorBrewer)

# Flag indicating whether or not to write plot to file or just display it in a new window.
WRITE_PNG = FALSE

# DPI, or dots per inch.
# A measure of the resolution of a printed document or digital scan.
DPI = 72 # 72 is fine for small plots. Use 144 for larger plots.

# Plot size.
PLOT_HEIGHT_PX = 350
PLOT_WIDTH_PX = 1000

################################################################################
# Number of discarded images as a function of prediction confidence threshold. #
################################################################################

pal = brewer.pal(n=3, name='Set2')

# Read CSV file containing prediction results and their confidences.
preds = read.csv(here('data', 'confidences.csv'))

# Correct predictions.
preds_correct = preds[preds$predicted_label == preds$expected_label,]

# Correct predictions per label.
preds_correct_bad = preds_correct[preds_correct$predicted_label == 'bad',]
preds_correct_earth = preds_correct[preds_correct$predicted_label == 'earth',]
preds_correct_edge = preds_correct[preds_correct$predicted_label == 'edge',]

# List the number of discarded correctly labeled thumbnails for different
# prediction confidence threshold steps.
discarded_correct_bad_counts = c()
discarded_correct_earth_counts = c()
discarded_correct_edge_counts = c()

# Incorrect predictions.
preds_incorrect = preds[preds$predicted_label != preds$expected_label,]

# Incorrect predictions per label.
preds_incorrect_bad = preds_incorrect[preds_incorrect$expected_label == 'bad',]
preds_incorrect_earth = preds_incorrect[preds_incorrect$expected_label == 'earth',]
preds_incorrect_edge = preds_incorrect[preds_incorrect$expected_label == 'edge',]

# List the number of discarded incorrectly labeled thumbnails for different
# prediction confidence threshold steps.
discarded_incorrect_bad_counts = c()
discarded_incorrect_earth_counts = c()
discarded_incorrect_edge_counts = c()
 
# Threshold for x-axis.
tresholds = seq(0, 1, 0.0005)

# Build discarded list.
for(threshold in tresholds){
  
  # Count number or correct predictions that are discarded given the current threshold.
  total_discarded_correct_bad = nrow(preds_correct_bad) - nrow(preds_correct_bad[preds_correct_bad$bad >= threshold,])
  total_discarded_correct_earth = nrow(preds_correct_earth) - nrow(preds_correct_earth[preds_correct_earth$earth >= threshold,])
  total_discarded_correct_edge = nrow(preds_correct_edge) - nrow(preds_correct_edge[preds_correct_edge$edge >= threshold,])
  
  # Build list of correct discard counts.
  discarded_correct_bad_counts = c(discarded_correct_bad_counts, total_discarded_correct_bad)
  discarded_correct_earth_counts = c(discarded_correct_earth_counts, total_discarded_correct_earth)
  discarded_correct_edge_counts = c(discarded_correct_edge_counts, total_discarded_correct_edge)
  
  # Count number or incorrect predictions that are discarded given the current threshold.
  total_discarded_incorrect_bad = nrow(preds_incorrect_bad) - nrow(preds_incorrect_bad[preds_incorrect_bad$bad >= threshold,])
  total_discarded_incorrect_earth = nrow(preds_incorrect_earth) - nrow(preds_incorrect_earth[preds_incorrect_earth$earth >= threshold,])
  total_discarded_incorrect_edge = nrow(preds_incorrect_edge) - nrow(preds_incorrect_edge[preds_incorrect_edge$edge >= threshold,])
  
  # Build list of incorrect discard counts.
  discarded_incorrect_bad_counts = c(discarded_incorrect_bad_counts, total_discarded_incorrect_bad)
  discarded_incorrect_earth_counts = c(discarded_incorrect_earth_counts, total_discarded_incorrect_earth)
  discarded_incorrect_edge_counts = c(discarded_incorrect_edge_counts, total_discarded_incorrect_edge)
}

# Write to file or display the plot.
if(WRITE_PNG == TRUE){
  png(here('plots', 'confidence_threshold.png'),
      height=PLOT_HEIGHT_PX, width=PLOT_WIDTH_PX, units='px', res=DPI)
}else{
  windows(height=PLOT_HEIGHT_PX/DPI, width=PLOT_WIDTH_PX/DPI)
}

# Remove title whitespace.
par(mar=c(5,4,1,2)+0.1)

# Plot.
plot(tresholds, discarded_correct_bad_counts, type='l', xaxt='n',
     ylab='Number of predictions below the threshold', xlab='Confidence threshold',
     col=pal[2], log="y", lwd=3, las=1)

# Axis with custom ticks so that we can include a tick for the confidence threshold.
axis(side=1, at=c(0, 0.2, 0.4, 0.48,0.6, 0.8, 1.0))


lines(tresholds, discarded_correct_earth_counts, col=pal[1], lwd=3)
lines(tresholds, discarded_correct_edge_counts, col=pal[3], lwd=3)

lines(tresholds, discarded_incorrect_bad_counts, col=pal[2], lty=4, lwd=3)
lines(tresholds, discarded_incorrect_earth_counts, col=pal[1], lty=4, lwd=3)
lines(tresholds, discarded_incorrect_edge_counts, col=pal[3], lty=4, lwd=3)

# Ideal confidence threshold line for given test set.
abline(v=0.48, lty=3, lwd=2, col='azure4')

# Legend box.
rect(-0.03, 55, 0.17, 1000, col='white')

# Legend title
text(0.072, 680, 'Images Classified', cex=1)

# Legend for correctly labeled images that are discarded.
legend(-0.027, 550, inset=0.01, lwd=2, title='Correctly',
       legend=c('Bad', 'Earth', 'Edge'),
       col=c(pal[2], pal[1], pal[3]),
       lty=1, cex=1, box.lty=0, seg.len=2)


# Legend for incorrectly labeled images that are discarded.
legend(0.07, 550, inset=0.01, lwd=2, title='Incorrectly',
       legend=c('Bad', 'Earth', 'Edge'),
       col=c(pal[2], pal[1], pal[3]),
       lty=4, cex=1, box.lty=0, seg.len=2)

# Device off.
if(WRITE_PNG == TRUE){
  dev.off()
}
